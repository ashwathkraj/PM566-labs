knitr::opts_chunk$set(eval = FALSE, include  = FALSE)
library(readr)
library(dplyr)
library(ggplot2)
mt_samples <- read_csv("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv")
mt_samples <- mt_samples %>%
select(description, medical_specialty, transcription)
head(mt_samples)
mt_samples$transcription[1]
mt_samples %>%
count(medical_specialty, sort = TRUE)
mt_samples %>%
unnest_tokens(token, text)
library(tidytext)
mt_samples %>%
unnest_tokens(token, text)
mt_samples %>%
unnest_tokens(' ', text)
mt_samples %>%
unnest_tokens(token, transcription
)
mt_samples %>%
unnest_tokens(token, transcription) %>%
count(token)
mt_samples %>%
unnest_tokens(token, transcription) %>%
count(token, sort=TRUE)
mt_samples %>%
unnest_tokens(token, transcription) %>%
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n,token)) +
geom_col()
mt_samples %>%
unnest_tokens(token, transcription) %>%
count(tokenE) %>%
top_n(20, n) %>%
ggplot(aes(n,token)) +
geom_col()
mt_samples %>%
unnest_tokens(token, transcription) %>%
count(token) %>%
top_n(20, n) %>%
ggplot(aes(n,token)) +
geom_col()
mt_samples %>%
unnest_tokens(token, transcription) %>%
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n,token)) +
geom
mt_samples %>%
unnest_tokens(token, transcription) %>%
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n,token)) +
geom_col()
mt_samples %>%
unnest_tokens(token, transcription) %>%
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n, fct_reorder(token,n))) +
geom_col()
install.packages('forcats')
library(forcats)
mt_samples %>%
unnest_tokens(token, transcription) %>%
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(n, fct_reorder(token,n))) +
geom_col()
ggplot(aes_auto(n))
mt_samples %>%
unnest_tokens(output=token, input = transcription) %>%
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
mt_samples %>%
unnest_tokens(output=token, input = transcription) %>%
anti_join(stop_words, by = c("token" = "word")) %>%
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
stop_words <- append(stop_words, [1,2,3,4,5,0])
stop_words <- append(stop_words, (1,2,3,4,5,0))
stop_words <- append(stop_words, (1 2 3 4 5 0))
stop_words <- append(stop_words, ("1")
stop_words <- append(stop_words, ("1"))
stop_words <- append(stop_words, values = 0:10))
stop_words <- append(stop_words, values = 0:10)
mt_samples %>%
unnest_tokens(output=token, input = transcription) %>%
anti_join(stop_words, by = c("token" = "word")) %>%
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
View(mt_samples)
View(stop_words)
stop_words <- rm(stop_words, values = 0:10)
View(stop_words)
View(stop_words)
rm(stop_words[[3]])
library(tidytext)
View(stop_words)
View(stop_words)
library(tidytext)
rm(stop_words)
mt_samples %>%
unnest_tokens(output=token, input = transcription) %>%
anti_join(stop_words, by = c("token" = "word")) %>%
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
View(mt_samples)
stop_words <- rm(stop_words[word], values = 0:10)
stop_words[word]
stop_words
stop_words$word
stop_words$word <- append(stopwords$word, values = 0:10)
stop_words$word <- append(stop_words$word, values = 0:10)
View(stop_words)
as.character(seq(0,100)
as.character(seq(0,100))
rm (stop_wrod)
rm (stop_words)
mt_samples %>%
unnest_tokens(word, input = transcription) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100))))
count(token, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
count(word, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
count(word, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
mt_samples %>%
unnest_tokens(word, input = transcription) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
mt_samples %>%
unnest_tokens(word, input = transcription) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
mt_samples %>%
unnest_ngrams(word, input = transcription, n = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
mt_samples %>%
unnest_ngrams(word, input = transcription, n = 3, n_min=2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
mt_samples %>%
unnest_ngrams(word, input = transcription, n = 3, n_min=2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
separate (ngram, into = c("word1","word2", "word3"), sep = " ") %>%
select(word1,word2,word3) %>%
filter(word2 == "patient") %>%
count(word2, sort=TRUE)
separate()
library(tidyverse)
mt_samples %>%
unnest_ngrams(word, input = transcription, n = 3, n_min=2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
separate (ngram, into = c("word1","word2", "word3"), sep = " ") %>%
select(word1,word2,word3) %>%
filter(word2 == "patient") %>%
count(word2, sort=TRUE)
mt_samples %>%
unnest_ngrams(n_gram, input = transcription, n = 3, n_min=2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
separate (ngram, into = c("word1","word2", "word3"), sep = " ") %>%
select(word1,word2,word3) %>%
filter(word2 == "patient") %>%
count(word2, sort=TRUE)
mt_samples %>%
unnest_ngrams(token, input = transcription, n = 3, n_min=2) %>%
separate (col = token, into = c("word1","word2", "word3"), sep = " ") %>%
select(word1,word2,word3) %>%
filter(word2 == "patient") %>%
count(word2, sort=TRUE)
mt_samples %>%
unnest_ngrams(token, input = transcription, n = 3, n_min=2) %>%
separate (col = token, into = c("word1","word2", "word3"), sep = " ") %>%
select(word1,word2,word3) %>%
filter(word2 == "patient") %>%
count(word2, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
mt_trigrams <- mt_samples %>%
unnest_ngrams(token, input = transcription, n = 3, n_min=2) %>%
separate (col = token, into = c("word1","word2", "word3"), sep = " ") %>%
select(word1,word2,word3)
View(mt_trigrams)
mt_trigrams %>%
filter(word2 == "blood") %>%
count (word1, sort=TRUE)
mt_trigrams %>%
count()
mt_trigrams %>%
count(word1,word2,word3, sort=TRUE)
mt_trigrams %>%
filter(word2 == "blood") %>%
count (word1, word2, word3, sort=TRUE)
mt_trigrams %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(word1,word2, sort= TRUE)
mt_trigrams %>%
filter(word2 == "blood") %>%
count (word1, word2, word3, sort=TRUE)
mt_samples %>%
unnest_tokens(token, transcription) %>%
group_by(medical_specialty) %>%
count(token) %>%
top_n(1,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(medical_specialty) %>%
count(token) %>%
top_n(1,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(medical_specialty) %>%
count(token, sort = TRUE) %>%
top_n(1,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(medical_specialty) %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(transcription[0:100]) %>%
count(token, sort = TRUE) %>%
top_n(5,n)
View(mt_samples)
View(mt_samples)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description[0:100]) %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "pain")
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "pain") %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "pain" || token == "pulmonary") %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "pain" | token == "pulmonary") %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "pain" & token == "pulmonary") %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "pain" | token == "pulmonary") %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "pain" | token == "pulmonary") %>%
count(token, sort = TRUE) %>%
top_n(1,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "pain" | token == "pulmonary") %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "pain") %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
count(token, sort = TRUE) %>%
top_n(5,n)
mt_samples %>%
unnest_tokens(token, transcription) %>%
anti_join(stop_words, by = c("token" = "word"))%>%
group_by(description) %>%
filter(token == "sleep") %>%
count(token, sort = TRUE) %>%
top_n(5,n)
